{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Report on \"Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper aims to provide better bound for the _nearest neighbor search_ (NNS) problem; in which given a set of n points $P = \\{p_{1},...,p_{n}\\}$ in an Euclidean space X with distance function d, we want to find the point in P closest to a query point q. Particularly, it focuses on the high-dimensional Euclidean space, since the low-dimensional case is already well solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of late, there has been an increased interest in taking an approximative approach to the nearest neighbor problem, that is,  finding the $\\epsilon$-_approximative nearest neighbors_ ($\\epsilon$-NNS). This is due to the unsatisfactory solutions currently available by means of exact algorithms, which provide little to no improvement over brute-force algorithms for high-dimensionality. This is a recurring problem within datascience known as \"the curse of dimensionality\". For a small enough value of $\\epsilon$, this approximation suffice for most practical purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research paper presents two new algorithms for approximating nearest neighbors in d-dimensional Euclidean space that significantly improved known bounds at the time of its publishing. One called the *bucket method*, which has preprocessing time polynomial in dimensionality (d) and sample size (n), but truly sublinear query time. And one making use of *locality-sensitive hashing* (LSH) functions, that has polynomial query time in log n and d, but mildly exponential preprocessing costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental results shows the first algorithm to be orders of magnitude faster in terms of running time over real data sets. These results are obtained by reducing the $\\epsilon$-NNS problem to a new problem, namely *point location in equal balls* (PLEB), which relies on a data-structure called *ring-cover trees*. For the second algorithm they introduce LSH and prove that the existence of such functions imply the existence of faster $\\epsilon$-NNS algorithms. Note that the second algorithm is purely theoretical, and merely implies the existence of a faster algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper presents two fundamental tools for tackling the nearest neighbor problem. The first one being the reduction of $\\epsilon$-NNS problem into another problem, namely *point location in equal balls*, which can be achieved by means of a data-structure called *ring-cover trees*, that allows for efficient searching, and the second which is the introduction of LSH.\n",
    "\n",
    "#### Reduction to Point Location in Equal Balls (PLEB)\n",
    "One of the main reasons behind the faster running time of the two new algorithms presented, is the reduction of the $\\epsilon$-NNS problem into a $\\epsilon$-PLEB problem. By relaxing the term nearest neighbor to *nearby* neighbor, this problem trades computational cost in exchange for result precision. The reduction is done through a set of definitions that \"translates\" the NNS problem into a PLEB problem. The general idea is to traverse the points in P, creating \"balls\" around each point p with radius r, $B(p,r)$. To query q, you can now check if it's intersecting any ball instead of calculating distances, simplifying the problem a lot.\n",
    "<br>\n",
    "The reduction from $\\epsilon$-NNS to $\\epsilon$-PLEB relies on the *ring-cover trees* data-structure, and can be done with only a small overhead in preprocessing.\n",
    "\n",
    "#### Ring-cover trees\n",
    "This structure allows us to find a *ring-separator* or *cover* for any point set in $P$, either of which allows us to decompose P into smaller sets $S_{1},...,S_{l}$, for which it is possible to quickly restrict the search to in P. This allows for efficient searching among our datapoints. The ring-cover tree data-structure can be constructed in $\\hat{O}(n^{2})$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "#### The bucketing method\n",
    "With the problem already being reduced from $\\epsilon$-NNS to $\\epsilon$-PLEB, the general idea is to apply a gitter over our dataspace P. For each ball $B_{i}$ in P, ball $\\bar{B}_{i}$ is defined at the set of grid cells that intersect it and are subsequently hashed. After this preprocessing step, q can be queried by computing the grid cell containing q and checking if it's stored in the hash table. This method solves the problem outlined in $O(n) \\times O(\\frac{1}{\\epsilon})^{d}$ preprocessing time and sublinear query time.\n",
    "\n",
    "#### Locality-Sensitive-Hashing (LSH)\n",
    "The paper also introduces the concept of *locality-sensitive hashing* which is a family of hashing function that have the property that datapoints close to each other will be hashed in the same buckets with a very high likelyhood. The general idea is to hash each datapoint in P using such a function, and now to query q, we can just hash q and check what others datapoints are hashed in the same bucket. Because of LSH, these datapoint will be close neighbors of q with a high probability. \n",
    "\n",
    "#### Alternative solutions\n",
    "The paper also goes to suggest other methods of tackling the problem in high-dimensional space; one of which is the reduction of dimensionality by projecting points into a random subspace of lower dimension, preserving distances within an error relative to $\\epsilon$. This leads to a query time only sublinear for large values of $\\epsilon$, which is unacceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall technical quality of this paper is high. All sources  are referenced and they provide detailed proofs for any new theorems or algorithms introduced. The algorithms are also explained in great detail, and only require minimal previous knowledge in order to understand.\n",
    "\n",
    "#### Flaws\n",
    "The abstract does not cover the entire scope of the paper. Some of the major discoveries are only introduced in the introduction. As an example, the reduction of the nearest neighbor problem to a point location in equal balls problem is of very high significance throughout the paper and may have even greater applicances in the future, but is not mentioned in the abstract whatsoever. This goes against the purpose of an abstract and what it is meant to communicate.\n",
    "\n",
    "#### Structure\n",
    "The structure of the paper is clear, and relevant concepts introduced in an order that makes sense, even for unexperienced readers. An example of this structure, is introducing the reduction of the problem as well as ring-cover trees before moving on to talk about the algorithms, since these concepts are preliminary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application and X-factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "The paper lists a variety of possible applications such as: data compression, databases, data mining, information retrieval, machine learning, pattern recognition and statistics. In reality, a lot - if not all - classification problems in higher dimensions can benefit from this research, since it outperforms current solutions to the same problem. The most significant part of the paper, in my opinion, is the introducton of LSH-functions. These have a multitude of applications ranging from recommendation systems to duplicate detection, which i will cover below.\n",
    "\n",
    "#### Recommendation systems\n",
    "Companies such as Spotify, Netflix and Youtube often recommend users content based on their history and preferences. This can be translated to a high-dimensional problem thousands of features realated to each type of content. Recommendations could either be based on songs that are similar to the users song choices, or on songs that users with similar taste listen to. This is also know as collaborative filtering (Liu, Haifeng, et al. 2014) .\n",
    "\n",
    "#### Jaccard similarity\n",
    "Near duplicate words can be found by calculating the *Jaccard similarity* between words, which is the edit distance between word. An approximative solution to this problem can be made using LSH (Mar√ßais, Guillaume et al. 2019). The usages of this technique could be things such as detecting plagiarism or creating spelling correction tools.\n",
    "\n",
    "### X-factor\n",
    "Some of the concepts introduced in this paper were very sensational at the time of publication. In general, reducing the know bounds of a well-known problem is a feat on its own. What i believe to be the most significant discoveries are the introduction of *locality-sensitive hashing* and the reduction of the problem to a *point location in equal balls* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can seem like the authors of this paper sometimes rushes to talk about time-complexity, definitions and proofs, which makes it hard to gain a fundamental understanding of what is being communicated. Realising the target audience of such a paper consists mainly of scientists, this aspect becomes negligable. However, for a 2nd year university student, the paper might be too complex to understand. \n",
    "<br><br>\n",
    "Apart from this, the general presentation is good as it is readable and subjects are ordered in a manner that is comprehensible for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Mar√ßais G., DeBlasio D., Pandey P., Kingsford C. 2019, 'Locality-sensitive hashing for the edit distance', *Bioinformatics*, vol 35, pp. 127-135.\n",
    "\n",
    "Liu H., Hu Z., Mian A., Tian H., & Zhu X. 2014, 'A new user similarity model to improve the accuracy of collaborative filtering', *Knowledge-Based Systems*, vol. 56, pp. 156-166."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
